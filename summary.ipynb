{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "from torchvision.models.vision_transformer import ViT_B_16_Weights\n",
    "from torchvision.models.vision_transformer import vit_b_16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ViT.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Layers | Hidden size $D$ | MLP size | Heads | Params |\n",
    "| :--- | :---: | :---: | :---: | :---: | :---: |\n",
    "| ViT-Base | 12 | 768 | 3072 | 12 | $86 \\mathrm{M}$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 12\n",
    "D = 768\n",
    "HEADS = 12\n",
    "\n",
    "PATCH = 16\n",
    "IMAGE_W = 224\n",
    "N = (IMAGE_W/PATCH)**2\n",
    "DH = D/HEADS # To keep num of params constant we set DH = D/HEADS\n",
    "DMLP = 3072 # 4 times the D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle 2D images, we reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_p \\in \\mathbb{R}^{N \\times\\left(P^2 \\cdot C\\right)}$,<br> where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}_p \\in \\mathbb{R}^{N \\times\\left(P^2 \\cdot C\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to BERT's [class] token, we prepend a learnable embedding to the sequence of embedded patches $\\left(\\mathbf{z}_0^0=\\mathbf{x}_{\\text {class }}\\right)$,<br>\n",
    "whose state at the output of the Transformer encoder $\\left(\\mathbf{z}_L^0\\right)$ serves as the image representation y.<br>\n",
    "Both during pre-training and fine-tuning, a classification head is attached to $\\mathbf{z}_L^0$.<br>\n",
    "The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layernorm (LN) is applied before every block, and residual connections after every block<br>\n",
    "The MLP contains two layers with a GELU non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\mathbf{z}_0 & =\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_p^1 \\mathbf{E} ; \\mathbf{x}_p^2 \\mathbf{E} ; \\cdots ; \\mathbf{x}_p^N \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^2 \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\\\\n",
    "\\mathbf{z}_{\\ell}^{\\prime} & =\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, & & \\ell=1 \\ldots L \\\\\n",
    "\\mathbf{z}_{\\ell} & =\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & & \\ell=1 \\ldots L \\\\\n",
    "\\mathbf{y} & =\\operatorname{LN}\\left(\\mathbf{z}_L^0\\right) & &\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard qkv self-attention (SA, Vaswani et al. (2017))<br>\n",
    "For each element in an input sequence $\\mathbf{z} \\in \\mathbb{R}^{N \\times D}$, we compute a weighted sum over all values $\\mathbf{v}$ in the sequence.<br> \n",
    "The attention weights $A_{i j}$ are based on the pairwise similarity between two elements of the sequence and their respective query $\\mathbf{q}^i$ and key $\\mathbf{k}^j$ representations.\n",
    "$$\n",
    "\\begin{array}{rlrl}\n",
    "{[\\mathbf{q}, \\mathbf{k}, \\mathbf{v}]} & =\\mathbf{z} \\mathbf{U}_{q k v} & \\mathbf{U}_{q k v} & \\in \\mathbb{R}^{D \\times 3 D_h}, \\\\\n",
    "A & =\\operatorname{softmax}\\left(\\mathbf{q} \\mathbf{k}^{\\top} / \\sqrt{D_h}\\right) & A \\in \\mathbb{R}^{N \\times N} \\\\\n",
    "\\mathrm{SA}(\\mathbf{z}) & =A \\mathbf{v} &\n",
    "\\end{array}\n",
    "$$\n",
    "Multihead self-attention (MSA) is an extension of SA in which we run $k$ self-attention operations, called \"heads\", in parallel, and project their concatenated outputs.<br>\n",
    "To keep compute and number of parameters constant when changing $k, D_h$ (Eq. 5 ) is typically set to $D / k$.\n",
    "$$\n",
    "\\operatorname{MSA}(\\mathbf{z})=\\left[\\mathrm{SA}_1(z) ; \\mathrm{SA}_2(z) ; \\cdots ; \\mathrm{SA}_k(z)\\right] \\mathbf{U}_{m s a} \\quad \\mathbf{U}_{m s a} \\in \\mathbb{R}^{k \\cdot D_h \\times D}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout, when used, is applied after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings.<br>\n",
    "Finally, all training is done on resolution 224."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to stay as close as possible to the original Transformer model, we made use of an additional [class] token, which is taken as image representation.<br>\n",
    "The output of this token is then transformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity in the single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ViT_B_16_Weights.DEFAULT\n",
    "w.get_state_dict(progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = w.get_state_dict(progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_m = vit_b_16()\n",
    "ref_m.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "class_token: [1, 1, D] = [1, 1, 768] second dimention is the num of embeddings, so it's always 1\n",
    "conv_proj: Conv2d(3, D, kernel_size=(16, 16), stride=(16, 16))\n",
    "conv_proj.weight: [B, 3, 224, 224] -> [OUT_CHAN, IN_CHAN, KERNEL, KERNEL] = [D, 3, P, P] = [768, 3, 16, 16] -> [B, 768, 224/16, 224/16] = [B, 768, 14, 14]\n",
    "conv_proj.bias: [768]\n",
    "flatten: [B, 768, 14, 14] -> [B, 768, 14 * 14] = [B, 768, 196]\n",
    "reshape [B, 768, 196] -> [B, 196, 768]\n",
    "encoder.pos_embedding: [1, N+1, D] = [1, 197, 768]\n",
    "dropout\n",
    "    encoder.layers.encoder_layer_0.ln_1: LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "    encoder.layers.encoder_layer_0.ln_1.weight: [D] [768] has learnable weights for each component of the vector\n",
    "    encoder_layer_0.ln_1.bias: [D] [768] \n",
    "    encoder.layers.encoder_layer_0.self_attention: \n",
    "        encoder.layers.encoder_layer_0.self_attention.in_proj_weight: [HEADS * DH * 3, D] = [2304, 768]\n",
    "        encoder.layers.encoder_layer_0.self_attention.in_proj_bias : [2304]\n",
    "        attention dropout is optoinal in pytorch's implementation, off by default. The paper doesn't have it\n",
    "        encoder.layers.encoder_layer_0.self_attention.out_proj.weight: [HEADS * DH, D] = [768, 768]\n",
    "        encoder.layers.encoder_layer_0.self_attention.out_proj.bias: [768]\n",
    "    dropout\n",
    "    residual\n",
    "    encoder.layers.encoder_layer_0.ln_2: LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "    encoder.layers.encoder_layer_0.mlp.linear_1: Linear(in_features=D, out_features=DMLP, bias=True)\n",
    "    dropout\n",
    "    encoder.layers.encoder_layer_0.mlp.linear_2: Linear(in_features=DMLP, out_features=D, bias=True)\n",
    "    dropout\n",
    "encoder.ln: LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
    "heads.head: Linear(in_features=768, out_features=CLASSES, bias=True)\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                      Kernel Shape              Input Shape               Output Shape              Param #\n",
       "================================================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        --                        [1, 3, 224, 224]          [1, 1000]                 768\n",
       "├─Conv2d (conv_proj)                                         [16, 16]                  [1, 3, 224, 224]          [1, 768, 14, 14]          590,592\n",
       "├─Encoder (encoder)                                          --                        [1, 197, 768]             [1, 197, 768]             151,296\n",
       "│    └─Dropout (dropout)                                     --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    └─Sequential (layers)                                   --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_1)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MultiheadAttention (self_attention)         --                        [1, 197, 768]             [1, 197, 768]             2,362,368\n",
       "│    │    │    └─Dropout (dropout)                           --                        [1, 197, 768]             [1, 197, 768]             --\n",
       "│    │    │    └─LayerNorm (ln_2)                            --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "│    │    │    └─MLPBlock (mlp)                              --                        [1, 197, 768]             [1, 197, 768]             4,722,432\n",
       "│    └─LayerNorm (ln)                                        --                        [1, 197, 768]             [1, 197, 768]             1,536\n",
       "├─Sequential (heads)                                         --                        [1, 768]                  [1, 1000]                 --\n",
       "│    └─Linear (head)                                         --                        [1, 768]                  [1, 1000]                 769,000\n",
       "================================================================================================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 173.23\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 104.09\n",
       "Params size (MB): 232.27\n",
       "Estimated Total Size (MB): 336.96\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ref_m, depth=4, input_size=(1, 3, IMAGE_W, IMAGE_W),col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\"], row_settings=[\"var_names\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_token                                                       encoder.layers.encoder_layer_0.ln_2.weight                        encoder.layers.encoder_layer_1.self_attention.out_proj.weight     encoder.layers.encoder_layer_2.self_attention.in_proj_weight      encoder.layers.encoder_layer_3.ln_1.weight                        encoder.layers.encoder_layer_3.mlp.linear_2.weight                encoder.layers.encoder_layer_4.mlp.linear_1.weight                encoder.layers.encoder_layer_5.ln_2.weight                        encoder.layers.encoder_layer_6.self_attention.out_proj.weight     encoder.layers.encoder_layer_7.self_attention.in_proj_weight      encoder.layers.encoder_layer_8.ln_1.weight                        encoder.layers.encoder_layer_8.mlp.linear_2.weight                encoder.layers.encoder_layer_9.mlp.linear_1.weight                encoder.layers.encoder_layer_10.ln_2.weight                       encoder.layers.encoder_layer_11.self_attention.out_proj.weight    heads.head.weight                                             \n",
      "conv_proj.weight                                                  encoder.layers.encoder_layer_0.ln_2.bias                          encoder.layers.encoder_layer_1.self_attention.out_proj.bias       encoder.layers.encoder_layer_2.self_attention.in_proj_bias        encoder.layers.encoder_layer_3.ln_1.bias                          encoder.layers.encoder_layer_3.mlp.linear_2.bias                  encoder.layers.encoder_layer_4.mlp.linear_1.bias                  encoder.layers.encoder_layer_5.ln_2.bias                          encoder.layers.encoder_layer_6.self_attention.out_proj.bias       encoder.layers.encoder_layer_7.self_attention.in_proj_bias        encoder.layers.encoder_layer_8.ln_1.bias                          encoder.layers.encoder_layer_8.mlp.linear_2.bias                  encoder.layers.encoder_layer_9.mlp.linear_1.bias                  encoder.layers.encoder_layer_10.ln_2.bias                         encoder.layers.encoder_layer_11.self_attention.out_proj.bias      heads.head.bias                                               \n",
      "conv_proj.bias                                                    encoder.layers.encoder_layer_0.mlp.linear_1.weight                encoder.layers.encoder_layer_1.ln_2.weight                        encoder.layers.encoder_layer_2.self_attention.out_proj.weight     encoder.layers.encoder_layer_3.self_attention.in_proj_weight      encoder.layers.encoder_layer_4.ln_1.weight                        encoder.layers.encoder_layer_4.mlp.linear_2.weight                encoder.layers.encoder_layer_5.mlp.linear_1.weight                encoder.layers.encoder_layer_6.ln_2.weight                        encoder.layers.encoder_layer_7.self_attention.out_proj.weight     encoder.layers.encoder_layer_8.self_attention.in_proj_weight      encoder.layers.encoder_layer_9.ln_1.weight                        encoder.layers.encoder_layer_9.mlp.linear_2.weight                encoder.layers.encoder_layer_10.mlp.linear_1.weight               encoder.layers.encoder_layer_11.ln_2.weight                                                                                     \n",
      "encoder.pos_embedding                                             encoder.layers.encoder_layer_0.mlp.linear_1.bias                  encoder.layers.encoder_layer_1.ln_2.bias                          encoder.layers.encoder_layer_2.self_attention.out_proj.bias       encoder.layers.encoder_layer_3.self_attention.in_proj_bias        encoder.layers.encoder_layer_4.ln_1.bias                          encoder.layers.encoder_layer_4.mlp.linear_2.bias                  encoder.layers.encoder_layer_5.mlp.linear_1.bias                  encoder.layers.encoder_layer_6.ln_2.bias                          encoder.layers.encoder_layer_7.self_attention.out_proj.bias       encoder.layers.encoder_layer_8.self_attention.in_proj_bias        encoder.layers.encoder_layer_9.ln_1.bias                          encoder.layers.encoder_layer_9.mlp.linear_2.bias                  encoder.layers.encoder_layer_10.mlp.linear_1.bias                 encoder.layers.encoder_layer_11.ln_2.bias                                                                                       \n",
      "encoder.layers.encoder_layer_0.ln_1.weight                        encoder.layers.encoder_layer_0.mlp.linear_2.weight                encoder.layers.encoder_layer_1.mlp.linear_1.weight                encoder.layers.encoder_layer_2.ln_2.weight                        encoder.layers.encoder_layer_3.self_attention.out_proj.weight     encoder.layers.encoder_layer_4.self_attention.in_proj_weight      encoder.layers.encoder_layer_5.ln_1.weight                        encoder.layers.encoder_layer_5.mlp.linear_2.weight                encoder.layers.encoder_layer_6.mlp.linear_1.weight                encoder.layers.encoder_layer_7.ln_2.weight                        encoder.layers.encoder_layer_8.self_attention.out_proj.weight     encoder.layers.encoder_layer_9.self_attention.in_proj_weight      encoder.layers.encoder_layer_10.ln_1.weight                       encoder.layers.encoder_layer_10.mlp.linear_2.weight               encoder.layers.encoder_layer_11.mlp.linear_1.weight                                                                             \n",
      "encoder.layers.encoder_layer_0.ln_1.bias                          encoder.layers.encoder_layer_0.mlp.linear_2.bias                  encoder.layers.encoder_layer_1.mlp.linear_1.bias                  encoder.layers.encoder_layer_2.ln_2.bias                          encoder.layers.encoder_layer_3.self_attention.out_proj.bias       encoder.layers.encoder_layer_4.self_attention.in_proj_bias        encoder.layers.encoder_layer_5.ln_1.bias                          encoder.layers.encoder_layer_5.mlp.linear_2.bias                  encoder.layers.encoder_layer_6.mlp.linear_1.bias                  encoder.layers.encoder_layer_7.ln_2.bias                          encoder.layers.encoder_layer_8.self_attention.out_proj.bias       encoder.layers.encoder_layer_9.self_attention.in_proj_bias        encoder.layers.encoder_layer_10.ln_1.bias                         encoder.layers.encoder_layer_10.mlp.linear_2.bias                 encoder.layers.encoder_layer_11.mlp.linear_1.bias                                                                               \n",
      "encoder.layers.encoder_layer_0.self_attention.in_proj_weight      encoder.layers.encoder_layer_1.ln_1.weight                        encoder.layers.encoder_layer_1.mlp.linear_2.weight                encoder.layers.encoder_layer_2.mlp.linear_1.weight                encoder.layers.encoder_layer_3.ln_2.weight                        encoder.layers.encoder_layer_4.self_attention.out_proj.weight     encoder.layers.encoder_layer_5.self_attention.in_proj_weight      encoder.layers.encoder_layer_6.ln_1.weight                        encoder.layers.encoder_layer_6.mlp.linear_2.weight                encoder.layers.encoder_layer_7.mlp.linear_1.weight                encoder.layers.encoder_layer_8.ln_2.weight                        encoder.layers.encoder_layer_9.self_attention.out_proj.weight     encoder.layers.encoder_layer_10.self_attention.in_proj_weight     encoder.layers.encoder_layer_11.ln_1.weight                       encoder.layers.encoder_layer_11.mlp.linear_2.weight                                                                             \n",
      "encoder.layers.encoder_layer_0.self_attention.in_proj_bias        encoder.layers.encoder_layer_1.ln_1.bias                          encoder.layers.encoder_layer_1.mlp.linear_2.bias                  encoder.layers.encoder_layer_2.mlp.linear_1.bias                  encoder.layers.encoder_layer_3.ln_2.bias                          encoder.layers.encoder_layer_4.self_attention.out_proj.bias       encoder.layers.encoder_layer_5.self_attention.in_proj_bias        encoder.layers.encoder_layer_6.ln_1.bias                          encoder.layers.encoder_layer_6.mlp.linear_2.bias                  encoder.layers.encoder_layer_7.mlp.linear_1.bias                  encoder.layers.encoder_layer_8.ln_2.bias                          encoder.layers.encoder_layer_9.self_attention.out_proj.bias       encoder.layers.encoder_layer_10.self_attention.in_proj_bias       encoder.layers.encoder_layer_11.ln_1.bias                         encoder.layers.encoder_layer_11.mlp.linear_2.bias                                                                               \n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj.weight     encoder.layers.encoder_layer_1.self_attention.in_proj_weight      encoder.layers.encoder_layer_2.ln_1.weight                        encoder.layers.encoder_layer_2.mlp.linear_2.weight                encoder.layers.encoder_layer_3.mlp.linear_1.weight                encoder.layers.encoder_layer_4.ln_2.weight                        encoder.layers.encoder_layer_5.self_attention.out_proj.weight     encoder.layers.encoder_layer_6.self_attention.in_proj_weight      encoder.layers.encoder_layer_7.ln_1.weight                        encoder.layers.encoder_layer_7.mlp.linear_2.weight                encoder.layers.encoder_layer_8.mlp.linear_1.weight                encoder.layers.encoder_layer_9.ln_2.weight                        encoder.layers.encoder_layer_10.self_attention.out_proj.weight    encoder.layers.encoder_layer_11.self_attention.in_proj_weight     encoder.ln.weight                                                                                                               \n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj.bias       encoder.layers.encoder_layer_1.self_attention.in_proj_bias        encoder.layers.encoder_layer_2.ln_1.bias                          encoder.layers.encoder_layer_2.mlp.linear_2.bias                  encoder.layers.encoder_layer_3.mlp.linear_1.bias                  encoder.layers.encoder_layer_4.ln_2.bias                          encoder.layers.encoder_layer_5.self_attention.out_proj.bias       encoder.layers.encoder_layer_6.self_attention.in_proj_bias        encoder.layers.encoder_layer_7.ln_1.bias                          encoder.layers.encoder_layer_7.mlp.linear_2.bias                  encoder.layers.encoder_layer_8.mlp.linear_1.bias                  encoder.layers.encoder_layer_9.ln_2.bias                          encoder.layers.encoder_layer_10.self_attention.out_proj.bias      encoder.layers.encoder_layer_11.self_attention.in_proj_bias       encoder.ln.bias                                                                                                                 \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "keys = np.array(list(state.keys()))\n",
    "\n",
    "max_len = -1\n",
    "max_i = -1\n",
    "for i, k in enumerate(keys):\n",
    "    if(len(k) >= max_len):\n",
    "        max_len = len(k)\n",
    "        max_i = i\n",
    "\n",
    "rows = 10\n",
    "pad_end = math.ceil(len(keys)/rows)*rows - len(keys)\n",
    "keys = np.pad(keys, (0,pad_end), constant_values='')\n",
    "\n",
    "margin  = 4\n",
    "lines = np.stack(np.array_split(keys, int(len(keys)/rows))).T\n",
    "for l in lines:\n",
    "    print((' ' * margin).join([str(key).ljust(max_len) for key in l]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
